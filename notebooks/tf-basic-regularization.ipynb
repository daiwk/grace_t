{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1/L2 regularization\n",
    "需要restart，再重新运行，不然placeholder的赋值会有问题。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, cross entropy on all data is 10.8458\n",
      "After 1000 training steps, cross entropy on all data is 6.21182\n",
      "After 2000 training steps, cross entropy on all data is 5.14197\n",
      "After 3000 training steps, cross entropy on all data is 2.63666\n",
      "After 4000 training steps, cross entropy on all data is 0.986491\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "def get_weight(shape, lamda):\n",
    "    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)\n",
    "#    tf.add_to_collection(\n",
    "#    \"losses\", tf.contrib.layers.l2_regularizer(lamda)(var))\n",
    "    return var\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2), name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name=\"y-input\")\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "layer_dimension = [2, 10, 10, 10, 1]\n",
    "\n",
    "n_layers = len(layer_dimension)\n",
    "\n",
    "cur_layer = x\n",
    "\n",
    "in_dimension = layer_dimension[0]\n",
    "\n",
    "for i in range(1, n_layers):\n",
    "    out_dimension = layer_dimension[i]\n",
    "    weight = get_weight([in_dimension, out_dimension], 0.0001)\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[out_dimension]))\n",
    "    cur_layer = tf.nn.relu(tf.matmul(cur_layer, weight) + bias)\n",
    "    in_dimension = layer_dimension[i]\n",
    "    \n",
    "y = cur_layer\n",
    "    \n",
    "mse_loss = tf.reduce_mean(tf.square(y_ - y))\n",
    "\n",
    "cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)) \n",
    "                               + (1 - y_) * tf.log(tf.clip_by_value(1-y, 1e-10, 1.0)))\n",
    "\n",
    "#tf.add_to_collection(\"losses\", mse_loss)\n",
    "\n",
    "tf.add_to_collection(\"losses\", cross_entropy)\n",
    "\n",
    "loss = tf.add_n(tf.get_collection(\"losses\"))\n",
    "\n",
    "\n",
    "dataset_size = 128\n",
    "\n",
    "rdm = RandomState(1)\n",
    "X = rdm.rand(dataset_size, 2)\n",
    "\n",
    "Y = [[int(x1 + x2 < 1)] for (x1, x2) in X]\n",
    "\n",
    "#Y = [[(x1 + x2) * 1.0] for (x1, x2) in X]\n",
    "\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "\n",
    "    \n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min(start + batch_size, dataset_size)\n",
    "        #print(X[start:end].shape)\n",
    "        #print(Y[start:end])\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 1000 == 0:\n",
    "            total_cross_entropy = sess.run(\n",
    "            loss, feed_dict={x: X, y_: Y})\n",
    "            print(\"After %d training steps, cross entropy on all data is %g\" % (i, total_cross_entropy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
