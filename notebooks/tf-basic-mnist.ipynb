{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-caf731c09742>:118: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "After 0 training step(s), validation accuracy using average model is 0.0904\n",
      "After 1000 training step(s), validation accuracy using average model is 0.9754\n",
      "After 2000 training step(s), validation accuracy using average model is 0.9822\n",
      "After 3000 training step(s), validation accuracy using average model is 0.9852\n",
      "After 4000 training step(s), validation accuracy using average model is 0.9842\n",
      "After 5000 training step(s), validation accuracy using average model is 0.9848\n",
      "After 6000 training step(s), validation accuracy using average model is 0.9858\n",
      "After 7000 training step(s), validation accuracy using average model is 0.9844\n",
      "After 8000 training step(s), validation accuracy using average model is 0.9844\n",
      "After 9000 training step(s), validation accuracy using average model is 0.9842\n",
      "After 10000 training step(s), validation accuracy using average model is 0.985\n",
      "After 11000 training step(s), validation accuracy using average model is 0.9844\n",
      "After 12000 training step(s), validation accuracy using average model is 0.985\n",
      "After 13000 training step(s), validation accuracy using average model is 0.9846\n",
      "After 14000 training step(s), validation accuracy using average model is 0.9846\n",
      "After 15000 training step(s), validation accuracy using average model is 0.9858\n",
      "After 16000 training step(s), validation accuracy using average model is 0.9854\n",
      "After 17000 training step(s), validation accuracy using average model is 0.9856\n",
      "After 18000 training step(s), validation accuracy using average model is 0.985\n",
      "After 19000 training step(s), validation accuracy using average model is 0.9846\n",
      "After 20000 training step(s), validation accuracy using average model is 0.9852\n",
      "After 21000 training step(s), validation accuracy using average model is 0.9848\n",
      "After 22000 training step(s), validation accuracy using average model is 0.9856\n",
      "After 23000 training step(s), validation accuracy using average model is 0.9854\n",
      "After 24000 training step(s), validation accuracy using average model is 0.9848\n",
      "After 25000 training step(s), validation accuracy using average model is 0.986\n",
      "After 26000 training step(s), validation accuracy using average model is 0.9852\n",
      "After 27000 training step(s), validation accuracy using average model is 0.9852\n",
      "After 28000 training step(s), validation accuracy using average model is 0.9864\n",
      "After 29000 training step(s), validation accuracy using average model is 0.9844\n",
      "After 30000 training step(s), test accuracy using average model is 0.984\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "LAYER1_NODE = 500\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "def inference(input_tensor, avg_class, reuse=False):\n",
    "    if avg_class == None:\n",
    "        with tf.variable_scope(\"layer1\", reuse=reuse):\n",
    "            weights1 = tf.get_variable(\"weights\", [INPUT_NODE, LAYER1_NODE], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            biases1 = tf.get_variable(\"biases\", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n",
    "            layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "\n",
    "        with tf.variable_scope(\"layer2\", reuse=reuse):\n",
    "            weights2 = tf.get_variable(\"weights\", [LAYER1_NODE, OUTPUT_NODE], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            biases2 = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n",
    "            layer2 = tf.matmul(layer1, weights2) + biases2\n",
    "            \n",
    "    else:\n",
    "        with tf.variable_scope(\"layer1\", reuse=reuse):\n",
    "            weights1 = tf.get_variable(\"weights\", [INPUT_NODE, LAYER1_NODE], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            biases1 = tf.get_variable(\"biases\", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n",
    "            layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "\n",
    "        with tf.variable_scope(\"layer2\", reuse=reuse):\n",
    "            weights2 = tf.get_variable(\"weights\", [LAYER1_NODE, OUTPUT_NODE], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            biases2 = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n",
    "            layer2 = tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "        \n",
    "    return layer2\n",
    "    \n",
    "#    if avg_class == None:\n",
    "#        return tf.matmul(layer1, weights2) + biases2\n",
    "#    else:\n",
    "#        ## 对weight和bias都用exponantial moving average\n",
    "#        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "#        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "    \n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name=\"y-input\")\n",
    "    \n",
    "    with tf.variable_scope(\"layer1\", reuse=False):\n",
    "        weights1 = tf.get_variable(\"weights\", [INPUT_NODE, LAYER1_NODE], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        biases1 = tf.get_variable(\"biases\", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    with tf.variable_scope(\"layer2\", reuse=False):\n",
    "        weights2 = tf.get_variable(\"weights\", [LAYER1_NODE, OUTPUT_NODE], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        biases2 = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "    y = inference(x, None, reuse=True)\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    variables_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    \n",
    "    variables_averages_op = variables_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    average_y = inference(x, variables_averages, reuse=True)\n",
    "    \n",
    "    # argmax: Returns the index with the largest value across axes of a tensor. \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    \n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    \n",
    "    loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE, \n",
    "        global_step, \n",
    "        mnist.train.num_examples / BATCH_SIZE, \n",
    "        LEARNING_RATE_DECAY)\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name=\"train\")\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        validate_feed = {x: mnist.validation.images, \n",
    "                        y_: mnist.validation.labels}\n",
    "        test_feed = {x: mnist.test.images, \n",
    "                    y_: mnist.test.labels}\n",
    "        \n",
    "        for i in range(TRAINING_STEPS):\n",
    "            if i % 1000 == 0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step(s), validation accuracy \"\n",
    "                     \"using average model is %g\" % (i, validate_acc))\n",
    "\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "        \n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print(\"After %d training step(s), test accuracy using average \"\n",
    "             \"model is %g\" % (TRAINING_STEPS, test_acc))\n",
    "        \n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"./mnist/data\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "tf.app.run()\n",
    "#if __name__ == \"__main__\":\n",
    "#    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
